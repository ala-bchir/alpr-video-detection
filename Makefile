# ============================================
# MAKEFILE - LabÃ©lisation des plaques avec Qwen VL
# ============================================
# Machine: 64GB RAM, RTX 5090 32GB VRAM
# ============================================

# Variables
IMAGE_NAME=sam3-video-processor
VLM_CONTAINER=qwen-labeler
PWD=$(shell pwd)

# ModÃ¨les disponibles (testÃ©s et validÃ©s)
QWEN_7B=Qwen/Qwen2.5-VL-7B-Instruct
QWEN_2B_OCR=prithivMLmods/Qwen2-VL-OCR-2B-Instruct
GLM_OCR=zai-org/GLM-OCR
ROLM_OCR=reducto/RolmOCR

.PHONY: build run all clean sort stop-vlm start-qwen-7b start-qwen-2b-ocr label-7b label-2b-ocr

# ============================================
# COMMANDES GÃ‰NÃ‰RALES
# ============================================

all: build run

build:
	docker build -t $(IMAGE_NAME) .

run:
	docker run --rm --gpus all \
		--ipc=host \
		--ulimit memlock=-1 \
		--ulimit stack=67108864 \
		-v $(PWD)/data/videos:/app/data/videos \
		-v $(PWD)/data/images:/app/data/images \
		-v $(PWD)/data/crop:/app/data/crop \
		-v $(PWD)/sam3_weights.pt:/app/sam3_weights.pt \
		-v $(PWD)/video_processor.py:/app/video_processor.py \
		$(IMAGE_NAME)

clean-outputs:
	sudo rm -rf /home/solayman/sam3/outputs/*.jpg

sort:
	docker run --rm \
		-v $(PWD)/data/crop:/app/data/crop \
		-v $(PWD)/data/clean_plates:/app/data/clean_plates \
		-v $(PWD)/data/rejected_plates:/app/data/rejected_plates \
		-v $(PWD)/auto_sort_plates.py:/app/auto_sort_plates.py \
		$(IMAGE_NAME) python /app/auto_sort_plates.py

# ============================================
# QWEN VL - Vision Language Models pour OCR
# ============================================

# ArrÃªter le serveur VLM actif
stop-vlm:
	@echo "ðŸ›‘ ArrÃªt du serveur VLM..."
	@docker stop $(VLM_CONTAINER) 2>/dev/null || true
	@docker rm $(VLM_CONTAINER) 2>/dev/null || true
	@echo "âœ… Serveur arrÃªtÃ©"

# -------------------------------------------
# QWEN 2.5-VL-7B (Haute prÃ©cision)
# VRAM: ~16GB | Vitesse: 145ms/image | PrÃ©cision: 99.9%
# -------------------------------------------
start-qwen-7b:
	@echo "ðŸš€ DÃ©marrage de Qwen2.5-VL-7B..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model $(QWEN_7B) \
		--max-model-len 2048 \
		--gpu-memory-utilization 0.9
	@echo "â³ Chargement (~2-5 min)... Logs: docker logs -f $(VLM_CONTAINER)"

label-7b:
	@echo "ðŸ“ LabÃ©lisation avec Qwen2.5-VL-7B..."
	pip install openai --break-system-packages -q 2>/dev/null || pip install openai -q
	python3 label_gen_qwen.py --model qwen25-vl-7b

# -------------------------------------------
# QWEN 2-VL-2B-OCR (Rapide - RecommandÃ©)
# VRAM: ~5GB | Vitesse: 91ms/image | PrÃ©cision: 98.2%
# -------------------------------------------
start-qwen-2b-ocr:
	@echo "ðŸš€ DÃ©marrage de Qwen2-VL-2B-OCR..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model $(QWEN_2B_OCR) \
		--max-model-len 2048 \
		--gpu-memory-utilization 0.9
	@echo "â³ Chargement (~1-2 min)... Logs: docker logs -f $(VLM_CONTAINER)"

label-2b-ocr:
	@echo "ðŸ“ LabÃ©lisation avec Qwen2-VL-2B-OCR..."
	pip install openai --break-system-packages -q 2>/dev/null || pip install openai -q
	python3 label_gen_qwen.py --model qwen2-vl-2b-ocr

# ============================================
# DATASET - Ground Truth de test
# ============================================

generate-gt:
	@echo "ðŸ“Š GÃ©nÃ©ration du ground truth..."
	python3 generate_ground_truth.py

# ============================================
# Ã‰VALUATION DES MODÃˆLES
# ============================================

eval-7b:
	@echo "ðŸ“Š Ã‰valuation de Qwen2.5-VL-7B..."
	python3 evaluate_models.py --model qwen25-vl-7b

eval-2b-ocr:
	@echo "ðŸ“Š Ã‰valuation de Qwen2-VL-2B-OCR..."
	python3 evaluate_models.py --model qwen2-vl-2b-ocr

# -------------------------------------------
# GLM-OCR (0.9B - Ultra lÃ©ger et spÃ©cialisÃ©)
# VRAM: ~2GB | Vitesse: TrÃ¨s rapide
# NÃ©cessite image custom avec Transformers nightly
# -------------------------------------------
build-glm-ocr:
	@echo "ðŸ”¨ Construction de l'image GLM-OCR..."
	docker build -t vllm-glm-ocr -f Dockerfile.glm-ocr .
	@echo "âœ… Image vllm-glm-ocr prÃªte"

start-glm-ocr:
	@echo "ðŸš€ DÃ©marrage de GLM-OCR..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm-glm-ocr \
		--model $(GLM_OCR) \
		--max-model-len 2048 \
		--gpu-memory-utilization 0.9 \
		--allowed-local-media-path /
	@echo "â³ Chargement (~1-2 min)... Logs: docker logs -f $(VLM_CONTAINER)"

eval-glm-ocr:
	@echo "ðŸ“Š Ã‰valuation de GLM-OCR..."
	python3 evaluate_models.py --model glm-ocr

# -------------------------------------------
# RolmOCR (7B - basÃ© sur Qwen2.5-VL)
# VRAM: ~16GB | OptimisÃ© pour OCR
# -------------------------------------------
start-rolm-ocr:
	@echo "ðŸš€ DÃ©marrage de RolmOCR..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model $(ROLM_OCR) \
		--max-model-len 2048 \
		--gpu-memory-utilization 0.9
	@echo "â³ Chargement (~2-5 min)... Logs: docker logs -f $(VLM_CONTAINER)"

eval-rolm-ocr:
	@echo "ðŸ“Š Ã‰valuation de RolmOCR..."
	python3 evaluate_models.py --model rolm-ocr

# -------------------------------------------
# Chandra (datalab-to/chandra)
# Utilise son propre conteneur Docker
# -------------------------------------------
start-chandra:
	@echo "ðŸš€ DÃ©marrage de Chandra..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model datalab-to/chandra \
		--max-model-len 2048 \
		--gpu-memory-utilization 0.9
	@echo "â³ Chargement... Logs: docker logs -f $(VLM_CONTAINER)"

eval-chandra:
	@echo "ðŸ“Š Ã‰valuation de Chandra..."
	python3 evaluate_models.py --model chandra

# -------------------------------------------
# LightOnOCR (1B - LÃ©ger et spÃ©cialisÃ© OCR)
# VRAM: ~3GB | Vitesse: Rapide
# -------------------------------------------
start-lighton-ocr:
	@echo "ðŸš€ DÃ©marrage de LightOnOCR-2-1B..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model lightonai/LightOnOCR-2-1B \
		--limit-mm-per-prompt '{"image": 1}' \
		--mm-processor-cache-gb 0 \
		--no-enable-prefix-caching
	@echo "â³ Chargement (~1-2 min)... Logs: docker logs -f $(VLM_CONTAINER)"

eval-lighton-ocr:
	@echo "ðŸ“Š Ã‰valuation de LightOnOCR..."
	python3 evaluate_models.py --model lighton-ocr

# -------------------------------------------
# Dots.OCR (rednote-hilab)
# -------------------------------------------
start-dots-ocr:
	@echo "ðŸš€ DÃ©marrage de Dots.OCR..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model rednote-hilab/dots.ocr \
		--max-model-len 2048 \
		--gpu-memory-utilization 0.9 \
		--trust-remote-code
	@echo "â³ Chargement... Logs: docker logs -f $(VLM_CONTAINER)"

eval-dots-ocr:
	@echo "ðŸ“Š Ã‰valuation de Dots.OCR..."
	python3 evaluate_models.py --model dots-ocr

# -------------------------------------------
# PaddleOCR-VL (PaddlePaddle)
# Prompt spÃ©cial: "OCR:"
# -------------------------------------------
start-paddle-ocr:
	@echo "ðŸš€ DÃ©marrage de PaddleOCR-VL..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:latest \
		--model PaddlePaddle/PaddleOCR-VL \
		--trust-remote-code \
		--max-num-batched-tokens 16384 \
		--no-enable-prefix-caching \
		--mm-processor-cache-gb 0
	@echo "â³ Chargement... Logs: docker logs -f $(VLM_CONTAINER)"

eval-paddle-ocr:
	@echo "ðŸ“Š Ã‰valuation de PaddleOCR-VL..."
	python3 evaluate_models.py --model paddle-ocr

# -------------------------------------------
# DeepSeek-OCR-2 (3B - nÃ©cessite vLLM nightly)
# -------------------------------------------
start-deepseek-ocr2:
	@echo "ðŸš€ DÃ©marrage de DeepSeek-OCR-2..."
	@docker run -d \
		--gpus all \
		-p 8000:8000 \
		--shm-size=32g \
		--name $(VLM_CONTAINER) \
		vllm/vllm-openai:nightly \
		--model deepseek-ai/DeepSeek-OCR-2 \
		--trust-remote-code \
		--max-model-len 4096 \
		--gpu-memory-utilization 0.9
	@echo "â³ Chargement... Logs: docker logs -f $(VLM_CONTAINER)"

eval-deepseek-ocr2:
	@echo "ðŸ“Š Ã‰valuation de DeepSeek-OCR-2..."
	python3 evaluate_models.py --model deepseek-ocr2

# ============================================
# PIPELINE COMPLET
# ============================================
# Usage: make pipeline START_TIME=12:30 TIME_WINDOW=3
# La vidÃ©o doit Ãªtre placÃ©e dans data/videos/ (une seule vidÃ©o)

VIDEOS_DIR=data/videos
TIME_WINDOW?=3

pipeline:
	@echo "ðŸŽ¬ Lancement du pipeline complet..."
	@if [ -z "$(START_TIME)" ]; then \
		echo "âŒ Erreur: START_TIME est requis"; \
		echo "Usage: make pipeline START_TIME=12:30 TIME_WINDOW=3"; \
		exit 1; \
	fi
	@VIDEO_FILE=$$(ls -1 $(VIDEOS_DIR)/*.mp4 $(VIDEOS_DIR)/*.avi $(VIDEOS_DIR)/*.mkv $(VIDEOS_DIR)/*.mov 2>/dev/null | head -1); \
	if [ -z "$$VIDEO_FILE" ]; then \
		echo "âŒ Aucune vidÃ©o trouvÃ©e dans $(VIDEOS_DIR)/"; \
		echo "ðŸ“ Formats supportÃ©s: .mp4, .avi, .mkv, .mov"; \
		exit 1; \
	fi; \
	echo "ðŸ“¹ VidÃ©o dÃ©tectÃ©e: $$VIDEO_FILE"; \
	echo "â° Heure de dÃ©part: $(START_TIME)"; \
	echo "ðŸ”„ FenÃªtre dÃ©dup: $(TIME_WINDOW)s"; \
	pip install requests opencv-python --break-system-packages -q 2>/dev/null || pip install requests opencv-python -q; \
	python3 pipeline.py --video "$$VIDEO_FILE" --start-time "$(START_TIME)" --time-window $(TIME_WINDOW)

# ============================================
# NETTOYAGE
# ============================================
clean:
	docker rmi $(IMAGE_NAME)